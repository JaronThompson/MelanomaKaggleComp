{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df.csv\").sample(frac=.3)\n",
    "val_df = pd.read_csv(\"val_df.csv\")\n",
    "path = \"../data/512x512-dataset-melanoma/512x512-dataset-melanoma/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional neural network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=7, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4)) \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1800, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load('model.ckpt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx', 'anatom_site_general_challenge'] \n",
    "\n",
    "encoder = {}\n",
    "for feature in meta_features: \n",
    "    # determine unique features  \n",
    "    categories = np.unique(np.array(train_df[feature].values, str))\n",
    "    for i, category in enumerate(categories): \n",
    "        if category != 'nan':\n",
    "            encoder[category] = np.float(i)\n",
    "encoder['nan'] = np.nan\n",
    "\n",
    "transform = transforms.Compose(\n",
    "                   [transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                    transforms.ToTensor()])\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, meta_data, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data loader.\n",
    "def make_weights_for_balanced_classes(df, nclasses=2):   \n",
    "    targets = df[\"target\"].values\n",
    "    count = [0] * nclasses                                                      \n",
    "    for label in targets:                                                         \n",
    "        count[label] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(targets)                                              \n",
    "    for idx, label in enumerate(targets):                                          \n",
    "        weight[idx] = weight_per_class[label]                                  \n",
    "    return weight  \n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = Dataset(train_df, path)\n",
    "train_weights = make_weights_for_balanced_classes(train_df)\n",
    "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_weights, len(train_weights))                                                \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           sampler = train_sampler)   \n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "'''\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, meta_data, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        labels = torch.reshape(labels, [len(labels), 1])\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        embedded = model.embedding(images)\n",
    "'''\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, meta_data, labels = data_iter.next()\n",
    "\n",
    "images = images.to(device)\n",
    "embed = model.embedding(images).detach().cpu().numpy()\n",
    "nn_pred = model(images).detach().cpu().numpy()\n",
    "\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((embed.shape[0], embed.shape[1]+meta_data.shape[1]+2))\n",
    "data[:, :embed.shape[1]] = embed \n",
    "data[:, embed.shape[1]:-2] = meta_data \n",
    "data[:, -2] = nn_pred.ravel()\n",
    "data[:, -1] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nans for now \n",
    "df = pd.DataFrame(data=data)\n",
    "data = np.array(df.dropna(), np.float)\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782608695652174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  7],\n",
       "       [ 3, 18]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
