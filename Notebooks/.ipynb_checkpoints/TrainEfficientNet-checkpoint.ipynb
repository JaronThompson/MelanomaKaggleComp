{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>tfrecord</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1872</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1872</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
       "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
       "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
       "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
       "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
       "\n",
       "  diagnosis benign_malignant  target  tfrecord  width  height  \n",
       "0   unknown           benign       0         0   6000    4000  \n",
       "1   unknown           benign       0         0   6000    4000  \n",
       "2     nevus           benign       0         6   1872    1053  \n",
       "3   unknown           benign       0         0   1872    1053  \n",
       "4   unknown           benign       0        11   6000    4000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload train dataframe\n",
    "train_df_allsamples = pd.read_csv(\"../../data-256/train.csv\")\n",
    "train_df_allsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = train_df_allsamples['patient_id'].values\n",
    "unique_patient_ids, counts_patient_ids = np.unique(patient_ids, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(counts_patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33126 training samples total.\n",
      "Only 1.763 percent of training data set is a true positive.\n",
      "Therefore, the baseline accuracy is 98.237\n"
     ]
    }
   ],
   "source": [
    "# create dictionary that maps image name to target \n",
    "image_names = train_df_allsamples[\"image_name\"].values \n",
    "targets = train_df_allsamples[\"target\"].values\n",
    "img_to_target = {image_name:target for image_name, target in zip(image_names, targets)}\n",
    "\n",
    "percent_tp = sum(targets)/len(targets) * 100 \n",
    "print(\"{} training samples total.\".format(len(targets)))\n",
    "print(\"Only {:.3f} percent of training data set is a true positive.\".format(percent_tp))\n",
    "print(\"Therefore, the baseline accuracy is {:.3f}\".format(np.max([percent_tp, 100-percent_tp])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27219 Training and 3025 Validation samples\n",
      "8.760 percent of validation data set is a positive.\n",
      "Baseline validation accuracy is 91.240\n"
     ]
    }
   ],
   "source": [
    "# update so that the number of positives balances negatives\n",
    "train_df_pos = train_df_allsamples.iloc[targets>0, :]\n",
    "train_df_neg = train_df_allsamples.iloc[targets==0, :]\n",
    "train_df_negsample = train_df_neg.sample(n=int(train_df_pos.shape[0]))\n",
    "\n",
    "# concatenate negative and positive samples, then shuffle using .sample() \n",
    "#train_val_df = pd.concat((train_df_pos, train_df_negsample)).sample(frac=1)\n",
    "train_val_df = train_df_allsamples.sample(frac=.25)\n",
    "\n",
    "train_val_split = .9\n",
    "n_train_val = train_val_df.shape[0]\n",
    "n_train = int(train_val_split*n_train_val)\n",
    "\n",
    "#train_df = train_val_df[:n_train]\n",
    "#val_df = train_val_df[n_train:]\n",
    "train_df = pd.read_csv(\"ENET_train_df.csv\")\n",
    "val_df = pd.read_csv(\"ENET_val_df.csv\")\n",
    "\n",
    "# create dictionary that maps image name to target \n",
    "image_names = val_df[\"image_id\"].values \n",
    "val_targets = val_df[\"target\"].values\n",
    "\n",
    "percent_tp = sum(val_targets)/len(val_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "\n",
    "print(\"{} Training and {} Validation samples\".format(n_train, n_train_val-n_train))\n",
    "print(\"{:.3f} percent of validation data set is a positive.\".format(percent_tp))\n",
    "print(\"Baseline validation accuracy is {:.3f}\".format(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx', 'anatom_site_general_challenge'] \n",
    "\n",
    "encoder = {}\n",
    "for feature in meta_features: \n",
    "    # determine unique features  \n",
    "    categories = np.unique(np.array(train_df[feature].values, str))\n",
    "    for i, category in enumerate(categories): \n",
    "        if category != 'nan':\n",
    "            encoder[category] = np.float(i)\n",
    "encoder['nan'] = np.nan\n",
    "\n",
    "# define a unique transform each time a positive is resampled: \n",
    "\n",
    "# basic transform \n",
    "transform_1 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# horizontal or vertical flip \n",
    "transform_2 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# no flip or rotation for test/validation data \n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(1.0, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def make_weights_for_balanced_classes(df, nclasses=2):   \n",
    "    targets = df[\"target\"].values\n",
    "    count = [0] * nclasses                                                      \n",
    "    for label in targets:                                                         \n",
    "        count[label] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(targets)                                              \n",
    "    for idx, label in enumerate(targets):                                          \n",
    "        weight[idx] = weight_per_class[label]   \n",
    "        \n",
    "    return np.array(weight)  \n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        label_encode = np.zeros(2)\n",
    "        label_encode[label] = 1\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_1(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n",
    "\n",
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        #label_encode = np.zeros(2)\n",
    "        #label_encode[label] = 1\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_valid(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n",
    "\n",
    "class MyDataLoader():\n",
    "    def __init__(self, df, path, batchsize, min_balance=None):\n",
    "        # store df, path, weights, ...\n",
    "        self.df = df \n",
    "        self.path = path\n",
    "        self.w = make_weights_for_balanced_classes(df)\n",
    "        self.batchsize = batchsize \n",
    "        self.balanced = True\n",
    "        self.min_balance = min_balance\n",
    "        \n",
    "        # create a dictionary to map image_ids to index and target in dataframe \n",
    "        image_ids = self.df['image_id'].values \n",
    "        self.targets = self.df['target'].values\n",
    "        inds = np.arange(len(image_ids))\n",
    "        self.imgID2Idx = {im_id:ind for (im_id, ind) in zip(image_ids, inds)}\n",
    "        self.imgID2Target = {im_id:target for (im_id, target) in zip(image_ids, self.targets)}\n",
    "        \n",
    "        # keep track of how many times samples have been drawn \n",
    "        self.counts = np.zeros(len(image_ids))\n",
    "        \n",
    "    def get_batch(self):\n",
    "        # get image ids for the batch \n",
    "        if np.sum(self.w > 0) >= self.batchsize:\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        else:\n",
    "            # update batchsize \n",
    "            print(\"Updating batchsize, maximum dataset size reached\")\n",
    "            self.batchsize = np.sum(self.w > 0)\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        \n",
    "        # get the index locations for the image ids \n",
    "        batch_sample_inds = [self.imgID2Idx[im_id] for im_id in batch_image_ids]\n",
    "        batch_targets = [self.imgID2Target[im_id] for im_id in batch_image_ids]\n",
    "        \n",
    "        # Update counts \n",
    "        self.counts[batch_sample_inds] += 1\n",
    "        \n",
    "        # Update sampling weights so that target=0 --> w = 0, target=1 --> w /= 2 \n",
    "        for ind, target in zip(batch_sample_inds, batch_targets):\n",
    "            # if the sample is a negative, then we don't want to sample it again \n",
    "            # if the sample has already been sampled 2 times, it shouldn't be sampled again\n",
    "            # if target is positive, sampling should happen less frequently \n",
    "            if target == 0 or self.counts[ind] == 2:\n",
    "                self.w[ind] = 0 \n",
    "            else:\n",
    "                self.w[ind] /= 2 \n",
    "        \n",
    "        # Data returned in shape [Batchsize, Channels, H, W]\n",
    "        images = np.zeros((self.batchsize, 3, 256, 256)) \n",
    "        labels = np.zeros(self.batchsize)\n",
    "        #meta_data = np.zeros((self.batchsize, 3))\n",
    "        \n",
    "        for i, index in enumerate(batch_sample_inds):\n",
    "            \n",
    "            # 1. load image\n",
    "            img_name = self.df['image_id'].values[index]\n",
    "            img_path = self.path + img_name + \".jpg\"\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "            img = Image.fromarray(img)\n",
    "            if self.counts[index] == 1:\n",
    "                images[i, :, :, :] = transform_1(img)\n",
    "            if self.counts[index] == 2:\n",
    "                images[i, :, :, :] = transform_2(img)\n",
    "\n",
    "            # 3. store label \n",
    "            labels[i] = self.imgID2Target[img_name]\n",
    "                \n",
    "        # Quit once all positive samples have zero valued weights \n",
    "        if np.sum(self.w[self.targets==1]) == 0:\n",
    "            self.balanced = False\n",
    "            \n",
    "        # If a min balance is specified, quit at min balance\n",
    "        if self.min_balance:\n",
    "            if sum(labels)/len(labels) <= self.min_balance:\n",
    "                self.balanced = False\n",
    "        \n",
    "        # return data \n",
    "        X = torch.tensor(images, dtype = torch.float32)\n",
    "        y = torch.tensor(labels, dtype = torch.float32)\n",
    "        return X, y #, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# First, load the EfficientNet with pre-trained parameters \n",
    "ENet = EfficientNet.from_pretrained('efficientnet-b0').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network\n",
    "class MyENet(nn.Module):\n",
    "    def __init__(self, ENet):\n",
    "        super(MyENet, self).__init__()\n",
    "        # modify output layer of the pre-trained ENet \n",
    "        self.ENet = ENet\n",
    "        num_ftrs = self.ENet._fc.in_features\n",
    "        self.ENet._fc = nn.Linear(in_features=num_ftrs, out_features=512)\n",
    "        # map Enet output to melanoma decision \n",
    "        self.output = nn.Sequential(nn.BatchNorm1d(512),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=0.2),\n",
    "                                    nn.Linear(512, 2), \n",
    "                                    nn.Softmax())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.ENet(x)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ENet(x)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "model = MyENet(ENet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-953f4e28bb35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-08b2f58e5d6e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\efficientnet_pytorch\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use the prebuilt data loader.\n",
    "path = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\n",
    "path_to_model = '../Models/ENETmodel.ckpt'\n",
    "\n",
    "batchsize  = 25\n",
    "# evaluate performance on validation data \n",
    "train_dataset = TrainDataset(train_df, path)                                              \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchsize) \n",
    "\n",
    "# evaluate performance on validation data \n",
    "valid_dataset = ValidDataset(val_df, path)                                              \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=1) \n",
    "\n",
    "# save losses from training \n",
    "train_roc = []\n",
    "losses    = []\n",
    "\n",
    "# save validation statistics \n",
    "val_roc = []\n",
    "val_acc = []\n",
    "val_precision = []\n",
    "val_recall = []\n",
    "\n",
    "patience     = 3\n",
    "set_patience = 3 \n",
    "best_val     = 0\n",
    "verbose = True\n",
    "\n",
    "# Loss and optimizer\n",
    "class_weights = compute_class_weight(y=train_df['target'].values, class_weight='balanced', classes=np.unique(train_df['target'].values))\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler reduces learning rate by factor of 10 when val auc does not improve\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, min_lr=3e-6, mode='max', patience=0, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # set up model for training \n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        '''\n",
    "        train_loader = MyDataLoader(train_df, path, batchsize=batchsize, min_balance=min_balance)\n",
    "        while train_loader.balanced:\n",
    "            images, labels = train_loader.get_batch()\n",
    "        '''\n",
    "        # make sure that there are some positive examples in the mini batch \n",
    "        n_pos = sum(labels)\n",
    "        if n_pos>0:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = weighted_binary_cross_entropy(outputs, labels, weights=class_weights)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # store loss\n",
    "            losses.append(loss)\n",
    "\n",
    "            if i%5==0:\n",
    "                # Calculate ROC\n",
    "                predictions = torch.max(outputs, 1).cpu().numpy()\n",
    "                targets = labels.cpu().numpy()\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(np.array(targets, np.int), np.array(predictions).ravel())\n",
    "                train_roc_auc = auc(fpr, tpr)\n",
    "                train_roc.append(train_roc_auc)\n",
    "\n",
    "                # Calculate balance \n",
    "                balance = np.sum(targets) / len(targets)\n",
    "\n",
    "                print ('Epoch [{}/{}], Balance {:.2f}, Loss: {:.4f}, Train ROC AUC: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, balance, loss.item(), train_roc_auc))\n",
    "        \n",
    "    # prep model for evaluation\n",
    "    valid_predictions = []\n",
    "    valid_targets = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for j, (images, labels) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            labels = torch.max(labels, 1)[1]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate val ROC\n",
    "            valid_predictions += [p[1] for p in outputs.detach().cpu().numpy()]\n",
    "            valid_targets += list(labels.cpu().numpy()) #[p[1] for p in labels.cpu().numpy()] \n",
    "\n",
    "    fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "    val_roc_epoch = auc(fpr, tpr)\n",
    "    val_roc.append(val_roc_epoch)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())).ravel()\n",
    "    val_acc_epoch = (tp + tn) / len(valid_targets)\n",
    "    val_precision_epoch = tp / (tp + fp)\n",
    "    val_recall_epoch = tp / (tp + fn)\n",
    "\n",
    "    val_acc.append(val_acc_epoch)\n",
    "    val_precision.append(val_precision_epoch)\n",
    "    val_recall.append(val_recall_epoch)\n",
    "\n",
    "    print ('\\nEpoch [{}/{}], Val ROC AUC: {:.3f}, Val Acc: {:.3f}, Val Precision: {:.3f}, Val Recall {:.3f}\\n'\n",
    "           .format(epoch+1, num_epochs, val_roc_epoch, val_acc_epoch, val_precision_epoch, val_recall_epoch))\n",
    "    \n",
    "    # learning rate is reduced if val roc doesn't improve \n",
    "    scheduler.step(val_roc_epoch)\n",
    "    \n",
    "    if val_roc_epoch > best_val:\n",
    "        best_val = val_roc_epoch\n",
    "        patience = set_patience        \n",
    "        torch.save(model.state_dict(), path_to_model)  \n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('Early stopping. Best validation roc_auc: {:.3f}'.format(best_val))\n",
    "            model.load_state_dict(torch.load(path_to_model), strict=False)\n",
    "            break\n",
    "\n",
    "# Load best model \n",
    "model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(losses,label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(val_roc, label = 'Validation ROC AUC')\n",
    "plt.plot(val_acc, label = 'Validation accuracy')\n",
    "plt.plot(val_recall, label = 'Validation recall')\n",
    "plt.plot(val_precision, label = 'Validation precision')\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = []\n",
    "valid_targets = []\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        valid_predictions += list(outputs.detach().cpu().numpy())\n",
    "        valid_targets += list(labels.cpu().numpy()) \n",
    "\n",
    "fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "percent_tp = sum(valid_targets)/len(valid_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "acc = 100 * np.sum(np.round(valid_predictions) == np.array(valid_targets)) / len(valid_targets)\n",
    "\n",
    "print('\\nBaseline classification accuracy: {:.2f}'.format(baseline))\n",
    "print('\\nModel classification accuracy:    {:.2f}'.format(acc))\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, \n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.round(np.ravel(valid_predictions)) == valid_targets) / len(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(valid_targets)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"Model accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Model precision: {:.2f}\".format(precision))\n",
    "print(\"Model recall: {:.2f}\".format(recall))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.hist(np.ravel(valid_predictions))\n",
    "plt.xlabel(\"P(y=malignant | x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"ENET_train_df.csv\", index=False)\n",
    "val_df.to_csv(\"ENET_val_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
