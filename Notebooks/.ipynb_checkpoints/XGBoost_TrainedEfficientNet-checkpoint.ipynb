{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 27219 images, validating on 3025 images.\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"ENET_train_df.csv\")\n",
    "val_df = pd.read_csv(\"ENET_val_df.csv\")\n",
    "path = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\n",
    "\n",
    "print(\"Training on {} images, validating on {} images.\".format(train_df.shape[0], val_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, load the EfficientNet with pre-trained parameters \n",
    "ENet = EfficientNet.from_pretrained('efficientnet-b0').to(device)\n",
    "\n",
    "# Convolutional neural network\n",
    "class MyENet(nn.Module):\n",
    "    def __init__(self, Net):\n",
    "        super(MyENet, self).__init__()\n",
    "        self.Net = Net\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(1000, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.Net(x)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.Net(x)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "model = MyENet(ENet).to(device)\n",
    "# Load best model \n",
    "model.load_state_dict(torch.load('../Models/ENETmodel_all.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx', 'anatom_site_general_challenge'] \n",
    "\n",
    "encoder = {}\n",
    "for feature in meta_features: \n",
    "    # determine unique features  \n",
    "    categories = np.unique(np.array(train_df[feature].values, str))\n",
    "    for i, category in enumerate(categories): \n",
    "        if category != 'nan':\n",
    "            encoder[category] = np.float(i)\n",
    "encoder['nan'] = np.nan\n",
    "\n",
    "# no flip or rotation for test/validation data \n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(1.0, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_valid(img)\n",
    "        \n",
    "        # 3. get meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, meta_data, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch_size = 12\\npath = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\\n\\ntrain_dataset = ValidDataset(train_df, path)                                               \\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \\n                                           batch_size=batch_size)   \\n\\nmodel = model.eval()\\nfor i, (images, meta_data, labels) in enumerate(tqdm(train_loader)):\\n    images = images.to(device)\\n\\n    # Forward pass\\n    embed = model.embedding(images)\\n    nn_pred = model.output(embed).detach().cpu().numpy()\\n    embedding = embed.detach().cpu().numpy()\\n\\n    # determine NN features for the set of images \\n    batch_features = np.concatenate((embedding, meta_data, nn_pred), axis=1)\\n    \\n    # append the dataset\\n    try:\\n        X = np.concatenate((X, batch_features), 0)\\n        y = np.append(y, labels.numpy())\\n    except:\\n        X = batch_features \\n        y = labels.numpy() \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the data loader.\n",
    "'''\n",
    "batch_size = 12\n",
    "path = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\n",
    "\n",
    "train_dataset = ValidDataset(train_df, path)                                               \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size)   \n",
    "\n",
    "model = model.eval()\n",
    "for i, (images, meta_data, labels) in enumerate(tqdm(train_loader)):\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    embed = model.embedding(images)\n",
    "    nn_pred = model.output(embed).detach().cpu().numpy()\n",
    "    embedding = embed.detach().cpu().numpy()\n",
    "\n",
    "    # determine NN features for the set of images \n",
    "    batch_features = np.concatenate((embedding, meta_data, nn_pred), axis=1)\n",
    "    \n",
    "    # append the dataset\n",
    "    try:\n",
    "        X = np.concatenate((X, batch_features), 0)\n",
    "        y = np.append(y, labels.numpy())\n",
    "    except:\n",
    "        X = batch_features \n",
    "        y = labels.numpy() \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X and y in pandas dataframe \n",
    "#XGB_data = pd.DataFrame(data=X)\n",
    "#XGB_data['targets'] = y \n",
    "#XGB_data.to_csv(\"XGB_ENET_train_all.csv\", index=False)\n",
    "XGB_data = pd.read_csv(\"XGB_ENET_train.csv\")\n",
    "\n",
    "X = np.array(XGB_data.values[:, :-1], np.float32) \n",
    "y = np.array(XGB_data['targets'].values, np.float32)\n",
    "\n",
    "# maybe normalize X? \n",
    "Xstd = (X - np.nanmean(X, 0)) / np.nanstd(X, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight positive examples more heavily \n",
    "def make_weights(targets):\n",
    "    nclasses = len(np.unique(targets))\n",
    "    count = [0] * nclasses                                                      \n",
    "    for label in targets:                                                         \n",
    "        count[np.int(label)] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(targets)                                              \n",
    "    for idx, label in enumerate(targets):                                          \n",
    "        weight[idx] = weight_per_class[np.int(label)]  \n",
    "        \n",
    "    return np.array(weight)\n",
    "\n",
    "# define function to fit and return xgboost model \n",
    "def fit_xgboost(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    # weight positive examples more heavily \n",
    "    w = make_weights(y_train)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train) #, weight=w)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val) \n",
    "\n",
    "    # booster params \n",
    "    param = {'n_estimators':5000, \n",
    "            'max_depth':16,\n",
    "            'learning_rate':0.02,\n",
    "            'subsample':0.8,\n",
    "            'eval_metric':'auc',\n",
    "            'objective': 'binary:logistic',\n",
    "            'nthread': 8}\n",
    "\n",
    "    # specify validation set \n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    # Training \n",
    "    num_round = 5000\n",
    "    bst = xgb.train(param, dtrain, num_round, evals=evallist, early_stopping_rounds=50)\n",
    "    ''' \n",
    "    clf = xgb.XGBClassifier( \n",
    "        n_estimators=5000,\n",
    "        max_depth=16, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4,\n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train, \n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=50)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "batch_size = 4\n",
    "valid_dataset = ValidDataset(val_df, path)                                               \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                           batch_size=batch_size)   \n",
    "\n",
    "model = model.eval()\n",
    "for i, (images, meta_data, labels) in enumerate(tqdm(valid_loader)):\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    embed = model.embedding(images)\n",
    "    nn_pred = model.output(embed).detach().cpu().numpy()\n",
    "    embedding = embed.detach().cpu().numpy()\n",
    "\n",
    "    # determine NN features for the set of images \n",
    "    batch_features = np.concatenate((embedding, meta_data.numpy(), nn_pred), axis=1)\n",
    "    \n",
    "    # append the dataset\n",
    "    try:\n",
    "        Xval = np.concatenate((Xval, batch_features), 0)\n",
    "        yval = np.append(yval, labels.numpy())\n",
    "    except:\n",
    "        Xval = batch_features \n",
    "        yval = labels.numpy() \n",
    "        \n",
    "XGB_data = pd.DataFrame(data=Xval)\n",
    "XGB_data['targets'] = yval \n",
    "XGB_data.to_csv(\"XGB_ENET_val_all.csv\", index=False)\n",
    "'''\n",
    "XGB_data = pd.read_csv(\"XGB_ENET_val.csv\")\n",
    "Xval = np.array(XGB_data.values[:, :-1], np.float32) \n",
    "yval = np.array(XGB_data['targets'].values, np.float32)\n",
    "\n",
    "Xval = (Xval - np.nanmean(X, 0)) / np.nanstd(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# Seems like stratified k-fold training and prediction is a very prominent strategy among high scoring models \n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits, shuffle=True)\n",
    "skf.get_n_splits(Xstd, y)\n",
    "\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.893432\n",
      "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-auc:0.912357\n",
      "[2]\tvalidation_0-auc:0.917782\n",
      "[3]\tvalidation_0-auc:0.919867\n",
      "[4]\tvalidation_0-auc:0.920396\n",
      "[5]\tvalidation_0-auc:0.921023\n",
      "[6]\tvalidation_0-auc:0.919\n",
      "[7]\tvalidation_0-auc:0.919164\n",
      "[8]\tvalidation_0-auc:0.91872\n",
      "[9]\tvalidation_0-auc:0.91943\n",
      "[10]\tvalidation_0-auc:0.919026\n",
      "[11]\tvalidation_0-auc:0.919504\n",
      "[12]\tvalidation_0-auc:0.920406\n",
      "[13]\tvalidation_0-auc:0.920058\n",
      "[14]\tvalidation_0-auc:0.92063\n",
      "[15]\tvalidation_0-auc:0.921224\n",
      "[16]\tvalidation_0-auc:0.921038\n",
      "[17]\tvalidation_0-auc:0.920617\n",
      "[18]\tvalidation_0-auc:0.920852\n",
      "[19]\tvalidation_0-auc:0.921042\n",
      "[20]\tvalidation_0-auc:0.921156\n",
      "[21]\tvalidation_0-auc:0.921079\n",
      "[22]\tvalidation_0-auc:0.921242\n",
      "[23]\tvalidation_0-auc:0.921277\n",
      "[24]\tvalidation_0-auc:0.921367\n",
      "[25]\tvalidation_0-auc:0.922296\n",
      "[26]\tvalidation_0-auc:0.922835\n",
      "[27]\tvalidation_0-auc:0.922853\n",
      "[28]\tvalidation_0-auc:0.923281\n",
      "[29]\tvalidation_0-auc:0.923377\n",
      "[30]\tvalidation_0-auc:0.923098\n",
      "[31]\tvalidation_0-auc:0.923421\n",
      "[32]\tvalidation_0-auc:0.923512\n",
      "[33]\tvalidation_0-auc:0.923878\n",
      "[34]\tvalidation_0-auc:0.923718\n",
      "[35]\tvalidation_0-auc:0.923896\n",
      "[36]\tvalidation_0-auc:0.924056\n",
      "[37]\tvalidation_0-auc:0.923841\n",
      "[38]\tvalidation_0-auc:0.923769\n",
      "[39]\tvalidation_0-auc:0.923771\n",
      "[40]\tvalidation_0-auc:0.923926\n",
      "[41]\tvalidation_0-auc:0.923918\n",
      "[42]\tvalidation_0-auc:0.923897\n",
      "[43]\tvalidation_0-auc:0.923763\n",
      "[44]\tvalidation_0-auc:0.923466\n",
      "[45]\tvalidation_0-auc:0.923296\n",
      "[46]\tvalidation_0-auc:0.923455\n",
      "[47]\tvalidation_0-auc:0.923409\n",
      "[48]\tvalidation_0-auc:0.923494\n",
      "[49]\tvalidation_0-auc:0.923772\n",
      "[50]\tvalidation_0-auc:0.923818\n",
      "[51]\tvalidation_0-auc:0.92368\n",
      "[52]\tvalidation_0-auc:0.923696\n",
      "[53]\tvalidation_0-auc:0.92367\n",
      "[54]\tvalidation_0-auc:0.923928\n",
      "[55]\tvalidation_0-auc:0.924141\n",
      "[56]\tvalidation_0-auc:0.924274\n",
      "[57]\tvalidation_0-auc:0.924611\n",
      "[58]\tvalidation_0-auc:0.924436\n",
      "[59]\tvalidation_0-auc:0.924543\n",
      "[60]\tvalidation_0-auc:0.92468\n",
      "[61]\tvalidation_0-auc:0.924874\n",
      "[62]\tvalidation_0-auc:0.924977\n",
      "[63]\tvalidation_0-auc:0.924964\n",
      "[64]\tvalidation_0-auc:0.925308\n",
      "[65]\tvalidation_0-auc:0.925396\n",
      "[66]\tvalidation_0-auc:0.925205\n",
      "[67]\tvalidation_0-auc:0.924977\n",
      "[68]\tvalidation_0-auc:0.925428\n",
      "[69]\tvalidation_0-auc:0.925696\n",
      "[70]\tvalidation_0-auc:0.925734\n",
      "[71]\tvalidation_0-auc:0.925516\n",
      "[72]\tvalidation_0-auc:0.9256\n",
      "[73]\tvalidation_0-auc:0.92565\n",
      "[74]\tvalidation_0-auc:0.925815\n",
      "[75]\tvalidation_0-auc:0.925854\n",
      "[76]\tvalidation_0-auc:0.925766\n",
      "[77]\tvalidation_0-auc:0.926092\n",
      "[78]\tvalidation_0-auc:0.925896\n",
      "[79]\tvalidation_0-auc:0.925918\n",
      "[80]\tvalidation_0-auc:0.926124\n",
      "[81]\tvalidation_0-auc:0.926628\n",
      "[82]\tvalidation_0-auc:0.926505\n",
      "[83]\tvalidation_0-auc:0.926573\n",
      "[84]\tvalidation_0-auc:0.926602\n",
      "[85]\tvalidation_0-auc:0.926743\n",
      "[86]\tvalidation_0-auc:0.92703\n",
      "[87]\tvalidation_0-auc:0.927137\n",
      "[88]\tvalidation_0-auc:0.92714\n",
      "[89]\tvalidation_0-auc:0.92715\n",
      "[90]\tvalidation_0-auc:0.927077\n",
      "[91]\tvalidation_0-auc:0.927213\n",
      "[92]\tvalidation_0-auc:0.927373\n",
      "[93]\tvalidation_0-auc:0.927242\n",
      "[94]\tvalidation_0-auc:0.92748\n",
      "[95]\tvalidation_0-auc:0.927307\n",
      "[96]\tvalidation_0-auc:0.92726\n",
      "[97]\tvalidation_0-auc:0.927331\n",
      "[98]\tvalidation_0-auc:0.927258\n",
      "[99]\tvalidation_0-auc:0.927242\n",
      "[100]\tvalidation_0-auc:0.927457\n",
      "[101]\tvalidation_0-auc:0.927583\n",
      "[102]\tvalidation_0-auc:0.927469\n",
      "[103]\tvalidation_0-auc:0.927532\n",
      "[104]\tvalidation_0-auc:0.927641\n",
      "[105]\tvalidation_0-auc:0.92776\n",
      "[106]\tvalidation_0-auc:0.927794\n",
      "[107]\tvalidation_0-auc:0.927783\n",
      "[108]\tvalidation_0-auc:0.927803\n",
      "[109]\tvalidation_0-auc:0.92774\n",
      "[110]\tvalidation_0-auc:0.927896\n",
      "[111]\tvalidation_0-auc:0.927945\n",
      "[112]\tvalidation_0-auc:0.928035\n",
      "[113]\tvalidation_0-auc:0.928008\n",
      "[114]\tvalidation_0-auc:0.928138\n",
      "[115]\tvalidation_0-auc:0.92837\n",
      "[116]\tvalidation_0-auc:0.928522\n",
      "[117]\tvalidation_0-auc:0.928594\n",
      "[118]\tvalidation_0-auc:0.92857\n",
      "[119]\tvalidation_0-auc:0.928641\n",
      "[120]\tvalidation_0-auc:0.928564\n",
      "[121]\tvalidation_0-auc:0.928551\n",
      "[122]\tvalidation_0-auc:0.928625\n",
      "[123]\tvalidation_0-auc:0.928737\n",
      "[124]\tvalidation_0-auc:0.928673\n",
      "[125]\tvalidation_0-auc:0.928589\n",
      "[126]\tvalidation_0-auc:0.928694\n",
      "[127]\tvalidation_0-auc:0.928754\n",
      "[128]\tvalidation_0-auc:0.928627\n",
      "[129]\tvalidation_0-auc:0.928673\n",
      "[130]\tvalidation_0-auc:0.928555\n",
      "[131]\tvalidation_0-auc:0.928857\n",
      "[132]\tvalidation_0-auc:0.928858\n",
      "[133]\tvalidation_0-auc:0.928947\n",
      "[134]\tvalidation_0-auc:0.928931\n",
      "[135]\tvalidation_0-auc:0.928786\n",
      "[136]\tvalidation_0-auc:0.928844\n",
      "[137]\tvalidation_0-auc:0.928818\n",
      "[138]\tvalidation_0-auc:0.928997\n",
      "[139]\tvalidation_0-auc:0.929042\n",
      "[140]\tvalidation_0-auc:0.929205\n",
      "[141]\tvalidation_0-auc:0.929262\n",
      "[142]\tvalidation_0-auc:0.929259\n",
      "[143]\tvalidation_0-auc:0.929243\n",
      "[144]\tvalidation_0-auc:0.929264\n",
      "[145]\tvalidation_0-auc:0.929308\n",
      "[146]\tvalidation_0-auc:0.92935\n",
      "[147]\tvalidation_0-auc:0.929269\n",
      "[148]\tvalidation_0-auc:0.929204\n",
      "[149]\tvalidation_0-auc:0.929386\n",
      "[150]\tvalidation_0-auc:0.929537\n",
      "[151]\tvalidation_0-auc:0.92942\n",
      "[152]\tvalidation_0-auc:0.92934\n",
      "[153]\tvalidation_0-auc:0.929361\n",
      "[154]\tvalidation_0-auc:0.929466\n",
      "[155]\tvalidation_0-auc:0.929508\n",
      "[156]\tvalidation_0-auc:0.929717\n",
      "[157]\tvalidation_0-auc:0.929647\n",
      "[158]\tvalidation_0-auc:0.929581\n",
      "[159]\tvalidation_0-auc:0.929626\n",
      "[160]\tvalidation_0-auc:0.929537\n",
      "[161]\tvalidation_0-auc:0.92966\n",
      "[162]\tvalidation_0-auc:0.929682\n",
      "[163]\tvalidation_0-auc:0.929664\n",
      "[164]\tvalidation_0-auc:0.929779\n",
      "[165]\tvalidation_0-auc:0.929971\n",
      "[166]\tvalidation_0-auc:0.93009\n",
      "[167]\tvalidation_0-auc:0.930073\n",
      "[168]\tvalidation_0-auc:0.93001\n",
      "[169]\tvalidation_0-auc:0.93007\n",
      "[170]\tvalidation_0-auc:0.930133\n",
      "[171]\tvalidation_0-auc:0.930137\n",
      "[172]\tvalidation_0-auc:0.93018\n",
      "[173]\tvalidation_0-auc:0.930336\n",
      "[174]\tvalidation_0-auc:0.930409\n",
      "[175]\tvalidation_0-auc:0.93054\n",
      "[176]\tvalidation_0-auc:0.93054\n",
      "[177]\tvalidation_0-auc:0.930699\n",
      "[178]\tvalidation_0-auc:0.9308\n",
      "[179]\tvalidation_0-auc:0.930844\n",
      "[180]\tvalidation_0-auc:0.930771\n",
      "[181]\tvalidation_0-auc:0.930886\n",
      "[182]\tvalidation_0-auc:0.930914\n",
      "[183]\tvalidation_0-auc:0.931\n",
      "[184]\tvalidation_0-auc:0.930969\n",
      "[185]\tvalidation_0-auc:0.930869\n",
      "[186]\tvalidation_0-auc:0.930926\n",
      "[187]\tvalidation_0-auc:0.930999\n",
      "[188]\tvalidation_0-auc:0.930965\n",
      "[189]\tvalidation_0-auc:0.931014\n",
      "[190]\tvalidation_0-auc:0.93104\n",
      "[191]\tvalidation_0-auc:0.93124\n",
      "[192]\tvalidation_0-auc:0.931316\n",
      "[193]\tvalidation_0-auc:0.931332\n",
      "[194]\tvalidation_0-auc:0.931346\n",
      "[195]\tvalidation_0-auc:0.931309\n",
      "[196]\tvalidation_0-auc:0.931366\n",
      "[197]\tvalidation_0-auc:0.931507\n",
      "[198]\tvalidation_0-auc:0.931535\n",
      "[199]\tvalidation_0-auc:0.931602\n",
      "[200]\tvalidation_0-auc:0.931595\n",
      "[201]\tvalidation_0-auc:0.931611\n",
      "[202]\tvalidation_0-auc:0.931665\n",
      "[203]\tvalidation_0-auc:0.931725\n",
      "[204]\tvalidation_0-auc:0.931768\n",
      "[205]\tvalidation_0-auc:0.931806\n",
      "[206]\tvalidation_0-auc:0.931809\n",
      "[207]\tvalidation_0-auc:0.931868\n",
      "[208]\tvalidation_0-auc:0.931833\n",
      "[209]\tvalidation_0-auc:0.931895\n",
      "[210]\tvalidation_0-auc:0.931906\n",
      "[211]\tvalidation_0-auc:0.931919\n",
      "[212]\tvalidation_0-auc:0.93186\n",
      "[213]\tvalidation_0-auc:0.931809\n",
      "[214]\tvalidation_0-auc:0.931828\n",
      "[215]\tvalidation_0-auc:0.931847\n",
      "[216]\tvalidation_0-auc:0.93195\n",
      "[217]\tvalidation_0-auc:0.93203\n",
      "[218]\tvalidation_0-auc:0.932036\n",
      "[219]\tvalidation_0-auc:0.9321\n",
      "[220]\tvalidation_0-auc:0.93215\n",
      "[221]\tvalidation_0-auc:0.932146\n",
      "[222]\tvalidation_0-auc:0.93216\n",
      "[223]\tvalidation_0-auc:0.932162\n",
      "[224]\tvalidation_0-auc:0.932186\n",
      "[225]\tvalidation_0-auc:0.932144\n",
      "[226]\tvalidation_0-auc:0.932092\n",
      "[227]\tvalidation_0-auc:0.932168\n",
      "[228]\tvalidation_0-auc:0.932124\n",
      "[229]\tvalidation_0-auc:0.932174\n",
      "[230]\tvalidation_0-auc:0.932278\n",
      "[231]\tvalidation_0-auc:0.932227\n",
      "[232]\tvalidation_0-auc:0.932302\n",
      "[233]\tvalidation_0-auc:0.932406\n",
      "[234]\tvalidation_0-auc:0.932475\n",
      "[235]\tvalidation_0-auc:0.932465\n",
      "[236]\tvalidation_0-auc:0.932513\n",
      "[237]\tvalidation_0-auc:0.932489\n",
      "[238]\tvalidation_0-auc:0.932514\n",
      "[239]\tvalidation_0-auc:0.932594\n",
      "[240]\tvalidation_0-auc:0.932626\n",
      "[241]\tvalidation_0-auc:0.932677\n",
      "[242]\tvalidation_0-auc:0.932708\n",
      "[243]\tvalidation_0-auc:0.932761\n",
      "[244]\tvalidation_0-auc:0.932739\n",
      "[245]\tvalidation_0-auc:0.93294\n",
      "[246]\tvalidation_0-auc:0.932927\n",
      "[247]\tvalidation_0-auc:0.932973\n",
      "[248]\tvalidation_0-auc:0.933013\n",
      "[249]\tvalidation_0-auc:0.933018\n",
      "[250]\tvalidation_0-auc:0.93306\n",
      "[251]\tvalidation_0-auc:0.933104\n",
      "[252]\tvalidation_0-auc:0.933141\n",
      "[253]\tvalidation_0-auc:0.933161\n",
      "[254]\tvalidation_0-auc:0.933177\n",
      "[255]\tvalidation_0-auc:0.933268\n",
      "[256]\tvalidation_0-auc:0.93334\n",
      "[257]\tvalidation_0-auc:0.933433\n",
      "[258]\tvalidation_0-auc:0.933482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259]\tvalidation_0-auc:0.933439\n",
      "[260]\tvalidation_0-auc:0.933402\n",
      "[261]\tvalidation_0-auc:0.933411\n",
      "[262]\tvalidation_0-auc:0.933434\n",
      "[263]\tvalidation_0-auc:0.933484\n",
      "[264]\tvalidation_0-auc:0.933479\n",
      "[265]\tvalidation_0-auc:0.933458\n",
      "[266]\tvalidation_0-auc:0.933473\n",
      "[267]\tvalidation_0-auc:0.933539\n",
      "[268]\tvalidation_0-auc:0.933714\n",
      "[269]\tvalidation_0-auc:0.933753\n",
      "[270]\tvalidation_0-auc:0.933761\n",
      "[271]\tvalidation_0-auc:0.933739\n",
      "[272]\tvalidation_0-auc:0.933731\n",
      "[273]\tvalidation_0-auc:0.933783\n",
      "[274]\tvalidation_0-auc:0.933801\n",
      "[275]\tvalidation_0-auc:0.933824\n",
      "[276]\tvalidation_0-auc:0.93392\n",
      "[277]\tvalidation_0-auc:0.933947\n",
      "[278]\tvalidation_0-auc:0.933964\n",
      "[279]\tvalidation_0-auc:0.934006\n",
      "[280]\tvalidation_0-auc:0.934047\n",
      "[281]\tvalidation_0-auc:0.934127\n",
      "[282]\tvalidation_0-auc:0.934174\n",
      "[283]\tvalidation_0-auc:0.934145\n",
      "[284]\tvalidation_0-auc:0.934198\n",
      "[285]\tvalidation_0-auc:0.934207\n",
      "[286]\tvalidation_0-auc:0.934259\n",
      "[287]\tvalidation_0-auc:0.934222\n",
      "[288]\tvalidation_0-auc:0.934247\n",
      "[289]\tvalidation_0-auc:0.934308\n",
      "[290]\tvalidation_0-auc:0.934387\n",
      "[291]\tvalidation_0-auc:0.93441\n",
      "[292]\tvalidation_0-auc:0.934403\n",
      "[293]\tvalidation_0-auc:0.934496\n",
      "[294]\tvalidation_0-auc:0.934571\n",
      "[295]\tvalidation_0-auc:0.93459\n",
      "[296]\tvalidation_0-auc:0.934578\n",
      "[297]\tvalidation_0-auc:0.93467\n",
      "[298]\tvalidation_0-auc:0.93468\n",
      "[299]\tvalidation_0-auc:0.9347\n",
      "[300]\tvalidation_0-auc:0.934765\n",
      "[301]\tvalidation_0-auc:0.934769\n",
      "[302]\tvalidation_0-auc:0.934867\n",
      "[303]\tvalidation_0-auc:0.934946\n",
      "[304]\tvalidation_0-auc:0.93497\n",
      "[305]\tvalidation_0-auc:0.935104\n",
      "[306]\tvalidation_0-auc:0.935159\n",
      "[307]\tvalidation_0-auc:0.935163\n",
      "[308]\tvalidation_0-auc:0.935148\n",
      "[309]\tvalidation_0-auc:0.935163\n",
      "[310]\tvalidation_0-auc:0.935163\n",
      "[311]\tvalidation_0-auc:0.935152\n",
      "[312]\tvalidation_0-auc:0.935166\n",
      "[313]\tvalidation_0-auc:0.9352\n",
      "[314]\tvalidation_0-auc:0.935274\n",
      "[315]\tvalidation_0-auc:0.935245\n",
      "[316]\tvalidation_0-auc:0.935307\n",
      "[317]\tvalidation_0-auc:0.935314\n",
      "[318]\tvalidation_0-auc:0.93531\n",
      "[319]\tvalidation_0-auc:0.93529\n",
      "[320]\tvalidation_0-auc:0.935304\n",
      "[321]\tvalidation_0-auc:0.935336\n",
      "[322]\tvalidation_0-auc:0.935319\n",
      "[323]\tvalidation_0-auc:0.935369\n",
      "[324]\tvalidation_0-auc:0.935318\n",
      "[325]\tvalidation_0-auc:0.935304\n",
      "[326]\tvalidation_0-auc:0.935361\n",
      "[327]\tvalidation_0-auc:0.935434\n",
      "[328]\tvalidation_0-auc:0.935484\n",
      "[329]\tvalidation_0-auc:0.935541\n",
      "[330]\tvalidation_0-auc:0.935572\n",
      "[331]\tvalidation_0-auc:0.935575\n",
      "[332]\tvalidation_0-auc:0.935548\n",
      "[333]\tvalidation_0-auc:0.935561\n",
      "[334]\tvalidation_0-auc:0.935551\n",
      "[335]\tvalidation_0-auc:0.935586\n",
      "[336]\tvalidation_0-auc:0.935607\n",
      "[337]\tvalidation_0-auc:0.935628\n",
      "[338]\tvalidation_0-auc:0.935639\n",
      "[339]\tvalidation_0-auc:0.935658\n",
      "[340]\tvalidation_0-auc:0.935706\n",
      "[341]\tvalidation_0-auc:0.935711\n",
      "[342]\tvalidation_0-auc:0.935778\n",
      "[343]\tvalidation_0-auc:0.935769\n",
      "[344]\tvalidation_0-auc:0.935798\n",
      "[345]\tvalidation_0-auc:0.935808\n",
      "[346]\tvalidation_0-auc:0.935845\n",
      "[347]\tvalidation_0-auc:0.935907\n",
      "[348]\tvalidation_0-auc:0.935868\n",
      "[349]\tvalidation_0-auc:0.9359\n",
      "[350]\tvalidation_0-auc:0.935926\n",
      "[351]\tvalidation_0-auc:0.935915\n",
      "[352]\tvalidation_0-auc:0.935952\n",
      "[353]\tvalidation_0-auc:0.935961\n",
      "[354]\tvalidation_0-auc:0.936014\n",
      "[355]\tvalidation_0-auc:0.935975\n",
      "[356]\tvalidation_0-auc:0.935957\n",
      "[357]\tvalidation_0-auc:0.936014\n",
      "[358]\tvalidation_0-auc:0.935996\n",
      "[359]\tvalidation_0-auc:0.936016\n",
      "[360]\tvalidation_0-auc:0.935969\n",
      "[361]\tvalidation_0-auc:0.936037\n",
      "[362]\tvalidation_0-auc:0.93602\n",
      "[363]\tvalidation_0-auc:0.936065\n",
      "[364]\tvalidation_0-auc:0.936094\n",
      "[365]\tvalidation_0-auc:0.936054\n",
      "[366]\tvalidation_0-auc:0.93603\n",
      "[367]\tvalidation_0-auc:0.936012\n",
      "[368]\tvalidation_0-auc:0.936043\n",
      "[369]\tvalidation_0-auc:0.936025\n",
      "[370]\tvalidation_0-auc:0.936022\n",
      "[371]\tvalidation_0-auc:0.936066\n",
      "[372]\tvalidation_0-auc:0.936067\n",
      "[373]\tvalidation_0-auc:0.93608\n",
      "[374]\tvalidation_0-auc:0.936122\n",
      "[375]\tvalidation_0-auc:0.936076\n",
      "[376]\tvalidation_0-auc:0.936142\n",
      "[377]\tvalidation_0-auc:0.936184\n",
      "[378]\tvalidation_0-auc:0.936173\n",
      "[379]\tvalidation_0-auc:0.936232\n",
      "[380]\tvalidation_0-auc:0.936212\n",
      "[381]\tvalidation_0-auc:0.936235\n",
      "[382]\tvalidation_0-auc:0.936254\n",
      "[383]\tvalidation_0-auc:0.936238\n",
      "[384]\tvalidation_0-auc:0.936238\n",
      "[385]\tvalidation_0-auc:0.936229\n",
      "[386]\tvalidation_0-auc:0.936227\n",
      "[387]\tvalidation_0-auc:0.936198\n",
      "[388]\tvalidation_0-auc:0.936223\n",
      "[389]\tvalidation_0-auc:0.93625\n",
      "[390]\tvalidation_0-auc:0.936247\n",
      "[391]\tvalidation_0-auc:0.936264\n",
      "[392]\tvalidation_0-auc:0.936314\n",
      "[393]\tvalidation_0-auc:0.936323\n",
      "[394]\tvalidation_0-auc:0.936318\n",
      "[395]\tvalidation_0-auc:0.936309\n",
      "[396]\tvalidation_0-auc:0.936354\n",
      "[397]\tvalidation_0-auc:0.936313\n",
      "[398]\tvalidation_0-auc:0.936344\n",
      "[399]\tvalidation_0-auc:0.936372\n",
      "[400]\tvalidation_0-auc:0.936389\n",
      "[401]\tvalidation_0-auc:0.936417\n",
      "[402]\tvalidation_0-auc:0.936424\n",
      "[403]\tvalidation_0-auc:0.936451\n",
      "[404]\tvalidation_0-auc:0.936491\n",
      "[405]\tvalidation_0-auc:0.936526\n",
      "[406]\tvalidation_0-auc:0.936521\n",
      "[407]\tvalidation_0-auc:0.936517\n",
      "[408]\tvalidation_0-auc:0.936492\n",
      "[409]\tvalidation_0-auc:0.936545\n",
      "[410]\tvalidation_0-auc:0.936554\n",
      "[411]\tvalidation_0-auc:0.936575\n",
      "[412]\tvalidation_0-auc:0.936567\n",
      "[413]\tvalidation_0-auc:0.936562\n",
      "[414]\tvalidation_0-auc:0.93658\n",
      "[415]\tvalidation_0-auc:0.936595\n",
      "[416]\tvalidation_0-auc:0.93662\n",
      "[417]\tvalidation_0-auc:0.93662\n",
      "[418]\tvalidation_0-auc:0.936632\n",
      "[419]\tvalidation_0-auc:0.936636\n",
      "[420]\tvalidation_0-auc:0.936643\n",
      "[421]\tvalidation_0-auc:0.936644\n",
      "[422]\tvalidation_0-auc:0.93663\n",
      "[423]\tvalidation_0-auc:0.936679\n",
      "[424]\tvalidation_0-auc:0.936669\n",
      "[425]\tvalidation_0-auc:0.936675\n",
      "[426]\tvalidation_0-auc:0.936718\n",
      "[427]\tvalidation_0-auc:0.936756\n",
      "[428]\tvalidation_0-auc:0.936754\n",
      "[429]\tvalidation_0-auc:0.936785\n",
      "[430]\tvalidation_0-auc:0.936746\n",
      "[431]\tvalidation_0-auc:0.936771\n",
      "[432]\tvalidation_0-auc:0.936813\n",
      "[433]\tvalidation_0-auc:0.936817\n",
      "[434]\tvalidation_0-auc:0.936815\n",
      "[435]\tvalidation_0-auc:0.936793\n",
      "[436]\tvalidation_0-auc:0.936786\n",
      "[437]\tvalidation_0-auc:0.936805\n",
      "[438]\tvalidation_0-auc:0.936854\n",
      "[439]\tvalidation_0-auc:0.936847\n",
      "[440]\tvalidation_0-auc:0.936885\n",
      "[441]\tvalidation_0-auc:0.936875\n",
      "[442]\tvalidation_0-auc:0.936884\n",
      "[443]\tvalidation_0-auc:0.936918\n",
      "[444]\tvalidation_0-auc:0.936907\n",
      "[445]\tvalidation_0-auc:0.936957\n",
      "[446]\tvalidation_0-auc:0.936963\n",
      "[447]\tvalidation_0-auc:0.936974\n",
      "[448]\tvalidation_0-auc:0.936948\n",
      "[449]\tvalidation_0-auc:0.936957\n",
      "[450]\tvalidation_0-auc:0.936924\n",
      "[451]\tvalidation_0-auc:0.936905\n"
     ]
    }
   ],
   "source": [
    "# define \"out of fold\" set of predictions, represents validation performance  \n",
    "oof = np.zeros(len(Xstd))\n",
    "ypred = np.zeros(len(Xval))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(Xstd, y)):\n",
    "    \n",
    "    # get data partitions for Xtrain and Xval\n",
    "    X_train, X_test = Xstd[train_index], Xstd[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # train xgboost \n",
    "    bst = fit_xgboost(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save out of fold predictions \n",
    "    probas = bst.predict_proba(X_test)\n",
    "    oof_pred = np.array([p[1] for p in probas])\n",
    "    oof[test_index] += oof_pred\n",
    "    \n",
    "    # save current model predictions on the true validation set \n",
    "    probas = bst.predict_proba(Xval)\n",
    "    val_pred = np.array([p[1] for p in probas])\n",
    "    ypred += val_pred / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = bst.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [p[1] for p in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y, oof)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, \n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Train ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(yval, ypred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, \n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Validation ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = accuracy_score(yval, np.round(ypred))\n",
    "print(\"validation accuracy: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval = np.array(XGB_data.values[:, :-1], np.float32) \n",
    "yval = np.array(XGB_data['targets'].values, np.float32)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(yval, np.round(Xval[:, -1])).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(yval)\n",
    "# precision is the fraction of correctly identified positive samples\n",
    "# precision asks:\"Of all the samples identified as positives, how many were correct?\"\n",
    "precision = tp / (tp + fp)\n",
    "# recall is the ability of the model to identify positive samples \n",
    "# recall asks:\"Of all the positive samples in the dataset, how many were identified by the model?\"\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"CNN Stats:\")\n",
    "\n",
    "print(\"Model accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Model precision: {:.2f}\".format(precision))\n",
    "print(\"Model recall: {:.2f}\".format(recall))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(yval, np.round(Xval[:, -1])))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(yval, np.round(ypred)).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(yval)\n",
    "# precision is the fraction of correctly identified positive samples\n",
    "# precision asks:\"Of all the samples identified as positives, how many were correct?\"\n",
    "precision = tp / (tp + fp)\n",
    "# recall is the ability of the model to identify positive samples \n",
    "# recall asks:\"Of all the positive samples in the dataset, how many were identified by the model?\"\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"\\nXGBoost Stats:\")\n",
    "\n",
    "print(\"Model accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Model precision: {:.2f}\".format(precision))\n",
    "print(\"Model recall: {:.2f}\".format(recall))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(yval, np.round(ypred)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(yval, np.round(Xval[:, -1]*(2/4) + ypred*(2/4))).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(yval)\n",
    "# precision is the fraction of correctly identified positive samples\n",
    "# precision asks:\"Of all the samples identified as positives, how many were correct?\"\n",
    "precision = tp / (tp + fp)\n",
    "# recall is the ability of the model to identify positive samples \n",
    "# recall asks:\"Of all the positive samples in the dataset, how many were identified by the model?\"\n",
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.hist(ypred, label='XGBoost')\n",
    "plt.hist(Xval[:, -1], label='CNN', alpha=.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "bst_feature_dict = bst.get_score(importance_type='gain')\n",
    "feature_names = list(bst_feature_dict.keys())\n",
    "feature_importance = [bst_feature_dict[key] for key in feature_names]\n",
    "feature_imp = pd.DataFrame()\n",
    "feature_imp['Feature'] = feature_names \n",
    "feature_imp['Value'] = feature_importance\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:20])\n",
    "plt.title('XGB95 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next try \"test time augmentation\" \n",
    "'''\n",
    "# \n",
    "transform_TTA = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.0), ratio=(0.8, 1.2)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class TTADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_TTA(img)\n",
    "        \n",
    "        # 3. get meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, meta_data, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n",
    "\n",
    "N_TTA = 5\n",
    "batch_size = 1\n",
    "valid_dataset = TTADataset(val_df, path)                                               \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                           batch_size=batch_size)   \n",
    "\n",
    "ypred_TTA = np.zeros((len(ypred), N_TTA))\n",
    "\n",
    "model = model.eval()\n",
    "for j in range(N_TTA):\n",
    "    for i, (images, meta_data, labels) in enumerate(tqdm(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        embed = model.embedding(images)\n",
    "        nn_pred = model.output(embed).detach().cpu().numpy()\n",
    "        embedding = embed.detach().cpu().numpy()\n",
    "\n",
    "        # determine NN features for the set of images \n",
    "        batch_features = np.concatenate((embedding, meta_data.numpy(), nn_pred), axis=1)\n",
    "\n",
    "        ypred_TTA[i, j] = bst.predict(xgb.DMatrix(batch_features))\n",
    "ypred_TTA_mean = np.mean(ypred_TTA, 1)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
