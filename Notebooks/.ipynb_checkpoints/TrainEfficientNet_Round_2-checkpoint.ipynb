{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id      image_id  target  source     sex  age_approx  \\\n",
       "0  IP_7279968  ISIC_2637011       0  ISIC20    male        45.0   \n",
       "1  IP_3075186  ISIC_0015719       0  ISIC20  female        45.0   \n",
       "2  IP_2842074  ISIC_0052212       0  ISIC20  female        50.0   \n",
       "3  IP_6890425  ISIC_0068279       0  ISIC20  female        45.0   \n",
       "4  IP_8723313  ISIC_0074268       0  ISIC20  female        55.0   \n",
       "\n",
       "  anatom_site_general_challenge  \n",
       "0                     head/neck  \n",
       "1               upper extremity  \n",
       "2               lower extremity  \n",
       "3                     head/neck  \n",
       "4               upper extremity  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload train dataframe\n",
    "train_df_allsamples = pd.read_csv(\"../TrainTestDataFrames/marking.csv\")\n",
    "train_df_allsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13610 Training and 3025 Validation samples\n",
      "8.793 percent of validation data set is a positive.\n",
      "Baseline validation accuracy is 91.207\n"
     ]
    }
   ],
   "source": [
    "# load train and val dataframes\n",
    "train_df = pd.read_csv(\"ENET_train_df.csv\").sample(frac=.5)\n",
    "val_df = pd.read_csv(\"ENET_val_df.csv\")\n",
    "\n",
    "n_train = train_df.shape[0]\n",
    "n_val = val_df.shape[0]\n",
    "\n",
    "# create dictionary that maps image name to target \n",
    "image_names = val_df[\"image_id\"].values \n",
    "val_targets = val_df[\"target\"].values\n",
    "\n",
    "percent_tp = sum(val_targets)/len(val_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "\n",
    "print(\"{} Training and {} Validation samples\".format(n_train, n_val))\n",
    "print(\"{:.3f} percent of validation data set is a positive.\".format(percent_tp))\n",
    "print(\"Baseline validation accuracy is {:.3f}\".format(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx', 'anatom_site_general_challenge'] \n",
    "\n",
    "encoder = {}\n",
    "for feature in meta_features: \n",
    "    # determine unique features  \n",
    "    categories = np.unique(np.array(train_df[feature].values, str))\n",
    "    for i, category in enumerate(categories): \n",
    "        if category != 'nan':\n",
    "            encoder[category] = np.float(i)\n",
    "encoder['nan'] = np.nan\n",
    "\n",
    "'''\n",
    "Training done in rounds, need different transforms in each round. \n",
    "\n",
    "round 1: Slight addition of noise + balanced training samples \n",
    "round 2: More noise + training sample balance matches test case\n",
    "\n",
    "'''\n",
    "\n",
    "# define a unique transform each time a positive is resampled: \n",
    "\n",
    "# define transform with more noise for round 2\n",
    "transform_round_2 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.2), ratio=(0.7, 1.3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# no flip or rotation for test/validation data\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(1.0, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    \n",
    "class RoundTwoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_round_2(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_valid(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# First, load the EfficientNet with pre-trained parameters \n",
    "ENet = EfficientNet.from_pretrained('efficientnet-b0').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional neural network\n",
    "class MyENet(nn.Module):\n",
    "    def __init__(self, ENet):\n",
    "        super(MyENet, self).__init__()\n",
    "        # modify output layer of the pre-trained ENet \n",
    "        self.ENet = ENet\n",
    "        num_ftrs = self.ENet._fc.in_features\n",
    "        self.ENet._fc = nn.Linear(in_features=num_ftrs, out_features=512, bias=True)\n",
    "        # map Enet output to melanoma decision \n",
    "        self.output = nn.Sequential(nn.BatchNorm1d(512),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=0.4),\n",
    "                                    nn.Linear(512, 1), \n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.ENet(x)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ENet(x)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "model = MyENet(ENet).to(device)\n",
    "model.load_state_dict(torch.load('../Models/ENETmodel.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50], Val ROC AUC: 0.863, Val Acc: 0.919, Val Precision: 0.652, Val Recall 0.162\n",
      "\n",
      "\n",
      "Epoch [2/50], Val ROC AUC: 0.839, Val Acc: 0.918, Val Precision: 0.846, Val Recall 0.083\n",
      "\n",
      "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Epoch [3/50], Val ROC AUC: 0.881, Val Acc: 0.927, Val Precision: 0.696, Val Recall 0.301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use the prebuilt data loader.\n",
    "path = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\n",
    "path_to_model = '../Models/ENETmodel_round_2.ckpt'\n",
    "\n",
    "# Set training params (batchsize limited by GPU memory)\n",
    "num_epochs = 50\n",
    "batchsize  = 25\n",
    "min_balance = .25\n",
    "\n",
    "# evaluate performance on unbalanced training data \n",
    "train_dataset = RoundTwoDataset(train_df, path)                                              \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size = batchsize) \n",
    "\n",
    "# evaluate performance on validation data \n",
    "valid_dataset = ValidDataset(val_df, path)                                              \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset) \n",
    "\n",
    "# save losses from training \n",
    "train_roc = []\n",
    "losses    = []\n",
    "\n",
    "# save validation statistics \n",
    "val_roc = []\n",
    "val_acc = []\n",
    "val_precision = []\n",
    "val_recall = []\n",
    "\n",
    "patience     = 3\n",
    "set_patience = 3 \n",
    "best_val     = 0\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler reduces learning rate by factor of 10 when val auc does not improve\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, min_lr=3e-6, mode='max', patience=0, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # set up model for training \n",
    "        model = model.train()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = torch.reshape(labels, [len(labels), 1])\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # store loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i%100 = 0:\n",
    "            # Calculate ROC\n",
    "            predictions = outputs.detach().cpu().numpy().ravel()\n",
    "            targets = labels.cpu().numpy().ravel()\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(np.array(targets, np.int), np.array(predictions).ravel())\n",
    "            train_roc_auc = auc(fpr, tpr)\n",
    "            train_roc.append(train_roc_auc)\n",
    "\n",
    "            # Calculate balance \n",
    "            balance = np.sum(targets) / len(targets)\n",
    "\n",
    "            print ('Epoch [{}/{}], Step {}, Loss: {:.4f}, Train ROC AUC: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, loss.item(), train_roc_auc))\n",
    "        \n",
    "    # prep model for evaluation\n",
    "    valid_predictions = []\n",
    "    valid_targets = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for j, (images, labels) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            labels = torch.reshape(labels, [len(labels), 1])\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate val ROC\n",
    "            valid_predictions += list(outputs.detach().cpu().numpy().ravel())\n",
    "            valid_targets += list(labels.cpu().numpy().ravel()) \n",
    "\n",
    "    fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "    val_roc_epoch = auc(fpr, tpr)\n",
    "    val_roc.append(val_roc_epoch)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())).ravel()\n",
    "    val_acc_epoch = (tp + tn) / len(valid_targets)\n",
    "    val_precision_epoch = tp / (tp + fp)\n",
    "    val_recall_epoch = tp / (tp + fn)\n",
    "    \n",
    "    val_acc.append(val_acc_epoch)\n",
    "    val_precision.append(val_precision_epoch)\n",
    "    val_recall.append(val_recall_epoch)\n",
    "    \n",
    "    print ('\\nEpoch [{}/{}], Val ROC AUC: {:.3f}, Val Acc: {:.3f}, Val Precision: {:.3f}, Val Recall {:.3f}\\n'\n",
    "           .format(epoch+1, num_epochs, val_roc_epoch, val_acc_epoch, val_precision_epoch, val_recall_epoch))\n",
    "    \n",
    "    # learning rate is reduced if val roc doesn't improve \n",
    "    scheduler.step(val_roc_epoch)\n",
    "    \n",
    "    if val_roc_epoch > best_val:\n",
    "        best_val = val_roc_epoch\n",
    "        patience = set_patience        \n",
    "        torch.save(model.state_dict(), path_to_model)  \n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('Early stopping. Best validation roc_auc: {:.3f}'.format(best_val))\n",
    "            model.load_state_dict(torch.load(path_to_model), strict=False)\n",
    "            break\n",
    "\n",
    "# Load best model \n",
    "model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(losses,label='Train loss')\n",
    "#plt.ylim([.4, .8])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(train_roc, label = 'Train ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(val_roc, label = 'Validation ROC AUC')\n",
    "plt.plot(val_acc, label = 'Validation accuracy')\n",
    "plt.plot(val_recall, label = 'Validation recall')\n",
    "plt.plot(val_precision, label = 'Validation precision')\n",
    "plt.plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = []\n",
    "valid_targets = []\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        labels = torch.reshape(labels, [len(labels), 1])\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        valid_predictions += list(outputs.detach().cpu().numpy().ravel())\n",
    "        valid_targets += list(labels.cpu().numpy().ravel()) \n",
    "\n",
    "fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "percent_tp = sum(valid_targets)/len(valid_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "acc = 100 * np.sum(np.round(valid_predictions) == np.array(valid_targets)) / len(valid_targets)\n",
    "\n",
    "print('\\nBaseline classification accuracy: {:.2f}'.format(baseline))\n",
    "print('\\nModel classification accuracy:    {:.2f}'.format(acc))\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, \n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(valid_targets)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"Model accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Model precision: {:.2f}\".format(precision))\n",
    "print(\"Model recall: {:.2f}\".format(recall))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.hist(valid_predictions)\n",
    "plt.xlabel(\"P(y=malignant | x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"ENET_train_df.csv\", index=False)\n",
    "#val_df.to_csv(\"ENET_val_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
