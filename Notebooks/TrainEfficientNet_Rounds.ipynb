{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id      image_id  target  source     sex  age_approx  \\\n",
       "0  IP_7279968  ISIC_2637011       0  ISIC20    male        45.0   \n",
       "1  IP_3075186  ISIC_0015719       0  ISIC20  female        45.0   \n",
       "2  IP_2842074  ISIC_0052212       0  ISIC20  female        50.0   \n",
       "3  IP_6890425  ISIC_0068279       0  ISIC20  female        45.0   \n",
       "4  IP_8723313  ISIC_0074268       0  ISIC20  female        55.0   \n",
       "\n",
       "  anatom_site_general_challenge  \n",
       "0                     head/neck  \n",
       "1               upper extremity  \n",
       "2               lower extremity  \n",
       "3                     head/neck  \n",
       "4               upper extremity  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload train dataframe\n",
    "train_df_allsamples = pd.read_csv(\"../TrainTestDataFrames/marking.csv\")\n",
    "train_df_allsamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54438 Training and 6049 Validation samples\n",
      "8.944 percent of validation data set is a positive.\n",
      "Baseline validation accuracy is 91.056\n"
     ]
    }
   ],
   "source": [
    "# load train and val dataframes\n",
    "train_df = pd.read_csv(\"ENET_train_df_all.csv\")\n",
    "val_df = pd.read_csv(\"ENET_val_df_all.csv\")\n",
    "\n",
    "n_train = train_df.shape[0]\n",
    "n_val = val_df.shape[0]\n",
    "\n",
    "# create dictionary that maps image name to target \n",
    "image_names = val_df[\"image_id\"].values \n",
    "val_targets = val_df[\"target\"].values\n",
    "\n",
    "percent_tp = sum(val_targets)/len(val_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "\n",
    "print(\"{} Training and {} Validation samples\".format(n_train, n_val))\n",
    "print(\"{:.3f} percent of validation data set is a positive.\".format(percent_tp))\n",
    "print(\"Baseline validation accuracy is {:.3f}\".format(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx', 'anatom_site_general_challenge'] \n",
    "\n",
    "encoder = {}\n",
    "for feature in meta_features: \n",
    "    # determine unique features  \n",
    "    categories = np.unique(np.array(train_df[feature].values, str))\n",
    "    for i, category in enumerate(categories): \n",
    "        if category != 'nan':\n",
    "            encoder[category] = np.float(i)\n",
    "encoder['nan'] = np.nan\n",
    "\n",
    "'''\n",
    "Training done in rounds, need different transforms in each round. \n",
    "\n",
    "round 1: Slight addition of noise \n",
    "round 2: More noise \n",
    "round 3: More noise\n",
    "\n",
    "'''\n",
    "\n",
    "# define a unique transform each time a positive is resampled: \n",
    "\n",
    "# ROUND 1 transforms \n",
    "\n",
    "# basic transform with noise \n",
    "transform_1_round1 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# horizontal or vertical flip \n",
    "transform_2_round1 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# ROUND 2 transforms \n",
    "\n",
    "# basic transform \n",
    "transform_1_round2 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.0), ratio=(0.7, 1.3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# horizontal or vertical flip \n",
    "transform_2_round2 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=32. / 255.,saturation=0.5),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.5, 1.0), ratio=(0.7, 1.3)),\n",
    "    transforms.RandomVerticalFlip(1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# no flip or rotation for test/validation data \n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(1.0, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def make_weights_for_balanced_classes(df, nclasses=2):   \n",
    "    targets = df[\"target\"].values\n",
    "    count = [0] * nclasses                                                      \n",
    "    for label in targets:                                                         \n",
    "        count[label] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(targets)                                              \n",
    "    for idx, label in enumerate(targets):                                          \n",
    "        weight[idx] = weight_per_class[label]   \n",
    "        \n",
    "    return np.array(weight)  \n",
    "\n",
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        \n",
    "        # load X \n",
    "        img_name = self.df['image_id'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        # determine meta data \n",
    "        meta = self.df[meta_features].values[index]\n",
    "        meta_data = np.array([encoder[str(m)] for m in meta])\n",
    "        \n",
    "        # load y \n",
    "        label = self.df[\"target\"].values[index]\n",
    "        target = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        img = Image.fromarray(img)\n",
    "        #img = img.resize((256, 256))\n",
    "        img_processed = transform_valid(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]\n",
    "\n",
    "# round 1 data loader \n",
    "class MyDataLoader_1():\n",
    "    def __init__(self, df, path, batchsize, min_balance=None):\n",
    "        # store df, path, weights, ...\n",
    "        self.df = df \n",
    "        self.path = path\n",
    "        self.w = make_weights_for_balanced_classes(df)\n",
    "        self.batchsize = batchsize \n",
    "        self.balanced = True\n",
    "        self.min_balance = min_balance\n",
    "        \n",
    "        # create a dictionary to map image_ids to index and target in dataframe \n",
    "        image_ids = self.df['image_id'].values \n",
    "        self.targets = self.df['target'].values\n",
    "        inds = np.arange(len(image_ids))\n",
    "        self.imgID2Idx = {im_id:ind for (im_id, ind) in zip(image_ids, inds)}\n",
    "        self.imgID2Target = {im_id:target for (im_id, target) in zip(image_ids, self.targets)}\n",
    "        \n",
    "        # keep track of how many times samples have been drawn \n",
    "        self.counts = np.zeros(len(image_ids))\n",
    "        \n",
    "    def get_batch(self):\n",
    "        # get image ids for the batch \n",
    "        if np.sum(self.w > 0) >= self.batchsize:\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        else:\n",
    "            # update batchsize \n",
    "            print(\"Updating batchsize, maximum dataset size reached\")\n",
    "            self.batchsize = np.sum(self.w > 0)\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        \n",
    "        # get the index locations for the image ids \n",
    "        batch_sample_inds = [self.imgID2Idx[im_id] for im_id in batch_image_ids]\n",
    "        batch_targets = [self.imgID2Target[im_id] for im_id in batch_image_ids]\n",
    "        \n",
    "        # Update counts \n",
    "        self.counts[batch_sample_inds] += 1\n",
    "        \n",
    "        # Update sampling weights so that target=0 --> w = 0, target=1 --> w /= 2 \n",
    "        for ind, target in zip(batch_sample_inds, batch_targets):\n",
    "            # if the sample is a negative, then we don't want to sample it again \n",
    "            # if the sample has already been sampled 2 times, it shouldn't be sampled again\n",
    "            # if target is positive, sampling should happen less frequently \n",
    "            if target == 0 or self.counts[ind] == 2:\n",
    "                self.w[ind] = 0 \n",
    "            else:\n",
    "                self.w[ind] /= 2 \n",
    "        \n",
    "        # Data returned in shape [Batchsize, Channels, H, W]\n",
    "        images = np.zeros((self.batchsize, 3, 256, 256)) \n",
    "        labels = np.zeros(self.batchsize)\n",
    "        #meta_data = np.zeros((self.batchsize, 3))\n",
    "        \n",
    "        for i, index in enumerate(batch_sample_inds):\n",
    "            \n",
    "            # 1. load image\n",
    "            img_name = self.df['image_id'].values[index]\n",
    "            img_path = self.path + img_name + \".jpg\"\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "            img = Image.fromarray(img)\n",
    "            if self.counts[index] == 1:\n",
    "                images[i, :, :, :] = transform_1_round1(img)\n",
    "            if self.counts[index] == 2:\n",
    "                images[i, :, :, :] = transform_2_round1(img)\n",
    "            \n",
    "            # 3. store label \n",
    "            labels[i] = self.imgID2Target[img_name]\n",
    "                \n",
    "        # Quit once all positive samples have zero valued weights \n",
    "        if np.sum(self.w[self.targets==1]) == 0:\n",
    "            self.balanced = False\n",
    "            \n",
    "        # If a min balance is specified, quit at min balance\n",
    "        if self.min_balance:\n",
    "            if sum(labels)/len(labels) <= self.min_balance:\n",
    "                self.balanced = False\n",
    "        \n",
    "        # return data \n",
    "        X = torch.tensor(images, dtype = torch.float32)\n",
    "        y = torch.tensor(labels, dtype = torch.float32)\n",
    "        return X, y #, meta_data\n",
    "    \n",
    "# round 2 data loader \n",
    "class MyDataLoader_2():\n",
    "    def __init__(self, df, path, batchsize, min_balance=None):\n",
    "        # store df, path, weights, ...\n",
    "        self.df = df \n",
    "        self.path = path\n",
    "        self.w = make_weights_for_balanced_classes(df)\n",
    "        self.batchsize = batchsize \n",
    "        self.balanced = True\n",
    "        self.min_balance = min_balance\n",
    "        \n",
    "        # create a dictionary to map image_ids to index and target in dataframe \n",
    "        image_ids = self.df['image_id'].values \n",
    "        self.targets = self.df['target'].values\n",
    "        inds = np.arange(len(image_ids))\n",
    "        self.imgID2Idx = {im_id:ind for (im_id, ind) in zip(image_ids, inds)}\n",
    "        self.imgID2Target = {im_id:target for (im_id, target) in zip(image_ids, self.targets)}\n",
    "        \n",
    "        # keep track of how many times samples have been drawn \n",
    "        self.counts = np.zeros(len(image_ids))\n",
    "        \n",
    "    def get_batch(self):\n",
    "        # get image ids for the batch \n",
    "        if np.sum(self.w > 0) >= self.batchsize:\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        else:\n",
    "            # update batchsize \n",
    "            print(\"Updating batchsize, maximum dataset size reached\")\n",
    "            self.batchsize = np.sum(self.w > 0)\n",
    "            batch_image_ids = self.df.sample(n=self.batchsize, weights=self.w)['image_id'].values\n",
    "        \n",
    "        # get the index locations for the image ids \n",
    "        batch_sample_inds = [self.imgID2Idx[im_id] for im_id in batch_image_ids]\n",
    "        batch_targets = [self.imgID2Target[im_id] for im_id in batch_image_ids]\n",
    "        \n",
    "        # Update counts \n",
    "        self.counts[batch_sample_inds] += 1\n",
    "        \n",
    "        # Update sampling weights so that target=0 --> w = 0, target=1 --> w /= 2 \n",
    "        for ind, target in zip(batch_sample_inds, batch_targets):\n",
    "            # if the sample is a negative, then we don't want to sample it again \n",
    "            # if the sample has already been sampled 2 times, it shouldn't be sampled again\n",
    "            # if target is positive, sampling should happen less frequently \n",
    "            if target == 0 or self.counts[ind] == 2:\n",
    "                self.w[ind] = 0 \n",
    "            else:\n",
    "                self.w[ind] /= 2 \n",
    "        \n",
    "        # Data returned in shape [Batchsize, Channels, H, W]\n",
    "        images = np.zeros((self.batchsize, 3, 256, 256)) \n",
    "        labels = np.zeros(self.batchsize)\n",
    "        #meta_data = np.zeros((self.batchsize, 3))\n",
    "        \n",
    "        for i, index in enumerate(batch_sample_inds):\n",
    "            \n",
    "            # 1. load image\n",
    "            img_name = self.df['image_id'].values[index]\n",
    "            img_path = self.path + img_name + \".jpg\"\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "            img = Image.fromarray(img)\n",
    "            if self.counts[index] == 1:\n",
    "                images[i, :, :, :] = transform_1_round2(img)\n",
    "            if self.counts[index] == 2:\n",
    "                images[i, :, :, :] = transform_2_round2(img)\n",
    "            \n",
    "            # 3. store label \n",
    "            labels[i] = self.imgID2Target[img_name]\n",
    "                \n",
    "        # Quit once all positive samples have zero valued weights \n",
    "        if np.sum(self.w[self.targets==1]) == 0:\n",
    "            self.balanced = False\n",
    "            \n",
    "        # If a min balance is specified, quit at min balance\n",
    "        if self.min_balance:\n",
    "            if sum(labels)/len(labels) <= self.min_balance:\n",
    "                self.balanced = False\n",
    "        \n",
    "        # return data \n",
    "        X = torch.tensor(images, dtype = torch.float32)\n",
    "        y = torch.tensor(labels, dtype = torch.float32)\n",
    "        return X, y #, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# First, load the EfficientNet with pre-trained parameters \n",
    "ENet = EfficientNet.from_pretrained('efficientnet-b0').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional neural network\n",
    "class MyENet(nn.Module):\n",
    "    def __init__(self, Net):\n",
    "        super(MyENet, self).__init__()\n",
    "        self.Net = Net\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(1000, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.Net(x)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.Net(x)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "model = MyENet(ENet).to(device)\n",
    "model.load_state_dict(torch.load('../Models/ENETmodel_all.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.2474, Train ROC AUC: 0.9610\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.4907, Train ROC AUC: 0.9167\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.2528, Train ROC AUC: 0.9679\n",
      "Round 2, Epoch [1/30], Balance 0.68, Loss: 0.4907, Train ROC AUC: 0.8088\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.2171, Train ROC AUC: 0.9740\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3890, Train ROC AUC: 0.9038\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.9657, Train ROC AUC: 0.7338\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4972, Train ROC AUC: 0.8933\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.2567, Train ROC AUC: 0.9615\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.3679, Train ROC AUC: 0.9091\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3721, Train ROC AUC: 0.9067\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.4882, Train ROC AUC: 0.8718\n",
      "Round 2, Epoch [1/30], Balance 0.28, Loss: 0.5489, Train ROC AUC: 0.8254\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.6116, Train ROC AUC: 0.8267\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.4724, Train ROC AUC: 0.9200\n",
      "Round 2, Epoch [1/30], Balance 0.64, Loss: 0.6785, Train ROC AUC: 0.7014\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.4218, Train ROC AUC: 0.8750\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.5183, Train ROC AUC: 0.7692\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4769, Train ROC AUC: 0.8933\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.5281, Train ROC AUC: 0.8681\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3683, Train ROC AUC: 0.9867\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.3604, Train ROC AUC: 0.9267\n",
      "Round 2, Epoch [1/30], Balance 0.68, Loss: 0.4247, Train ROC AUC: 0.9265\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3115, Train ROC AUC: 0.9933\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4146, Train ROC AUC: 0.8867\n",
      "Round 2, Epoch [1/30], Balance 0.64, Loss: 0.6243, Train ROC AUC: 0.8056\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.4581, Train ROC AUC: 0.8590\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.6733, Train ROC AUC: 0.7597\n",
      "Round 2, Epoch [1/30], Balance 0.56, Loss: 0.4084, Train ROC AUC: 0.8571\n",
      "Round 2, Epoch [1/30], Balance 0.68, Loss: 0.4675, Train ROC AUC: 0.9338\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.5261, Train ROC AUC: 0.8269\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.6408, Train ROC AUC: 0.7500\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4149, Train ROC AUC: 0.9200\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.4606, Train ROC AUC: 0.8611\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.4668, Train ROC AUC: 0.8819\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.3408, Train ROC AUC: 0.9615\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.5001, Train ROC AUC: 0.8052\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.6026, Train ROC AUC: 0.8133\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.3463, Train ROC AUC: 0.9610\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4111, Train ROC AUC: 0.9067\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.3407, Train ROC AUC: 0.9545\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.5269, Train ROC AUC: 0.8400\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.4356, Train ROC AUC: 0.8533\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3949, Train ROC AUC: 0.8974\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.6180, Train ROC AUC: 0.7403\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3706, Train ROC AUC: 0.9400\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3667, Train ROC AUC: 0.9231\n",
      "Round 2, Epoch [1/30], Balance 0.64, Loss: 0.7095, Train ROC AUC: 0.7778\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3906, Train ROC AUC: 0.9000\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.5935, Train ROC AUC: 0.7115\n",
      "Round 2, Epoch [1/30], Balance 0.56, Loss: 0.3582, Train ROC AUC: 0.9026\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.3809, Train ROC AUC: 0.8961\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.6507, Train ROC AUC: 0.7867\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3550, Train ROC AUC: 0.9231\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.3252, Train ROC AUC: 0.9333\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3237, Train ROC AUC: 0.9615\n",
      "Round 2, Epoch [1/30], Balance 0.60, Loss: 0.5007, Train ROC AUC: 0.8333\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.6486, Train ROC AUC: 0.8403\n",
      "Round 2, Epoch [1/30], Balance 0.64, Loss: 0.5973, Train ROC AUC: 0.7847\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.6632, Train ROC AUC: 0.8403\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.5927, Train ROC AUC: 0.8125\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.3825, Train ROC AUC: 0.9038\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.3367, Train ROC AUC: 1.0000\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.7365, Train ROC AUC: 0.6538\n",
      "Round 2, Epoch [1/30], Balance 0.32, Loss: 0.3726, Train ROC AUC: 0.9338\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.5420, Train ROC AUC: 0.8462\n",
      "Round 2, Epoch [1/30], Balance 0.64, Loss: 0.4860, Train ROC AUC: 0.9653\n",
      "Round 2, Epoch [1/30], Balance 0.36, Loss: 0.3698, Train ROC AUC: 0.9236\n",
      "Round 2, Epoch [1/30], Balance 0.40, Loss: 0.3630, Train ROC AUC: 0.9333\n",
      "Round 2, Epoch [1/30], Balance 0.44, Loss: 0.3737, Train ROC AUC: 0.8961\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.4695, Train ROC AUC: 0.8654\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.3546, Train ROC AUC: 0.9038\n",
      "Round 2, Epoch [1/30], Balance 0.48, Loss: 0.6572, Train ROC AUC: 0.8269\n",
      "Round 2, Epoch [1/30], Balance 0.32, Loss: 0.3554, Train ROC AUC: 0.9191\n",
      "Round 2, Epoch [1/30], Balance 0.52, Loss: 0.4108, Train ROC AUC: 0.8654\n",
      "Round 2, Epoch [1/30], Balance 0.24, Loss: 0.4163, Train ROC AUC: 0.9825\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use the prebuilt data loader.\n",
    "path = \"../../data-512/512x512-dataset-melanoma/512x512-dataset-melanoma/\"\n",
    "path_to_model = '../Models/ENETmodel_all_2.ckpt'\n",
    "\n",
    "# evaluate performance on validation data \n",
    "valid_dataset = ValidDataset(val_df, path)                                              \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset) \n",
    "\n",
    "# save losses from training \n",
    "num_epochs = 30\n",
    "batchsize  = 25\n",
    "\n",
    "train_roc = []\n",
    "val_roc   = []\n",
    "losses    = []\n",
    "patience     = 3\n",
    "set_patience = 3 \n",
    "best_val     = 0\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "### ROUND 2 ### \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# scheduler reduces learning rate by factor of 10 when val auc does not improve\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, min_lr=3e-6, mode='max', patience=0, verbose=True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loader = MyDataLoader_2(train_df, path, batchsize=batchsize, min_balance=.25)\n",
    "    while train_loader.balanced:\n",
    "        images, labels = train_loader.get_batch()\n",
    "\n",
    "        # set up model for training \n",
    "        model = model.train()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = torch.reshape(labels, [len(labels), 1])\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # store loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Calculate ROC\n",
    "        predictions = outputs.detach().cpu().numpy().ravel()\n",
    "        targets = labels.cpu().numpy().ravel()\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(np.array(targets, np.int), np.array(predictions).ravel())\n",
    "        train_roc_auc = auc(fpr, tpr)\n",
    "        train_roc.append(train_roc_auc)\n",
    "        \n",
    "        # Calculate balance \n",
    "        balance = np.sum(targets) / len(targets)\n",
    "        \n",
    "        print ('Round 2, Epoch [{}/{}], Balance {:.2f}, Loss: {:.4f}, Train ROC AUC: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, balance, loss.item(), train_roc_auc))\n",
    "        \n",
    "    # prep model for evaluation\n",
    "    valid_predictions = []\n",
    "    valid_targets = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for j, (images, labels) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            labels = torch.reshape(labels, [len(labels), 1])\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate val ROC\n",
    "            valid_predictions += list(outputs.detach().cpu().numpy().ravel())\n",
    "            valid_targets += list(labels.cpu().numpy().ravel()) \n",
    "\n",
    "    fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "    val_roc_auc = auc(fpr, tpr)\n",
    "    val_roc.append(val_roc_auc)\n",
    "    \n",
    "    print ('\\nRound 2, Epoch [{}/{}], Val ROC AUC: {:.4f}\\n'.format(epoch+1, num_epochs, val_roc_auc))\n",
    "    \n",
    "    # learning rate is reduced if val roc doesn't improve \n",
    "    scheduler.step(val_roc_auc)\n",
    "    \n",
    "    if val_roc_auc >= best_val:\n",
    "        best_val = val_roc_auc\n",
    "        patience = set_patience        \n",
    "        torch.save(model.state_dict(), path_to_model)  \n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print('Early stopping. Best validation roc_auc: {:.3f}'.format(best_val))\n",
    "            model.load_state_dict(torch.load(path_to_model), strict=False)\n",
    "            break\n",
    "\n",
    "# Load best model \n",
    "model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(losses,label='Train loss')\n",
    "#plt.ylim([.4, .8])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(train_roc, label = 'Train ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(val_roc, label = 'Validation ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = []\n",
    "valid_targets = []\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        labels = torch.reshape(labels, [len(labels), 1])\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        valid_predictions += list(outputs.detach().cpu().numpy().ravel())\n",
    "        valid_targets += list(labels.cpu().numpy().ravel()) \n",
    "\n",
    "fpr, tpr, _ = roc_curve(np.array(valid_targets, np.int), np.array(valid_predictions).ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "percent_tp = sum(valid_targets)/len(valid_targets) * 100 \n",
    "baseline = np.max([percent_tp, 100-percent_tp])\n",
    "acc = 100 * np.sum(np.round(valid_predictions) == np.array(valid_targets)) / len(valid_targets)\n",
    "\n",
    "print('\\nBaseline classification accuracy: {:.2f}'.format(baseline))\n",
    "print('\\nModel classification accuracy:    {:.2f}'.format(acc))\n",
    "\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, \n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / len(valid_targets)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"Model accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Model precision: {:.2f}\".format(precision))\n",
    "print(\"Model recall: {:.2f}\".format(recall))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(confusion_matrix(np.array(valid_targets, np.int), np.round(np.array(valid_predictions).ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams.update({'font.size': 16, \n",
    "                     'legend.framealpha':1, \n",
    "                     'legend.edgecolor':'inherit'}) \n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.hist(valid_predictions)\n",
    "plt.xlabel(\"P(y=malignant | x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"ENET_train_df.csv\", index=False)\n",
    "#val_df.to_csv(\"ENET_val_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
