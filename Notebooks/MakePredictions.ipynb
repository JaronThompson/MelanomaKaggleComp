{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "#import xgboost as xgb\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (GPU can be enabled in settings)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0052060</td>\n",
       "      <td>IP_3579794</td>\n",
       "      <td>male</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0052349</td>\n",
       "      <td>IP_7782715</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0058510</td>\n",
       "      <td>IP_7960270</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0073313</td>\n",
       "      <td>IP_6375035</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>torso</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0073502</td>\n",
       "      <td>IP_0589375</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_0052060  IP_3579794    male        70.0                           NaN   \n",
       "1  ISIC_0052349  IP_7782715    male        40.0               lower extremity   \n",
       "2  ISIC_0058510  IP_7960270  female        55.0                         torso   \n",
       "3  ISIC_0073313  IP_6375035  female        50.0                         torso   \n",
       "4  ISIC_0073502  IP_0589375  female        45.0               lower extremity   \n",
       "\n",
       "   width  height  \n",
       "0   6000    4000  \n",
       "1   6000    4000  \n",
       "2   6000    4000  \n",
       "3   6000    4000  \n",
       "4   1920    1080  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload train dataframe\n",
    "path = \"../../data-256/\"\n",
    "test_df = pd.read_csv(path + 'test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no flip or rotation for test/validation data \n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path_to_files):\n",
    "        # 1. Initialize file paths or a list of file names.\n",
    "        self.path = path_to_files\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "\n",
    "        # load X\n",
    "        img_name = self.df['image_name'].values[index]\n",
    "        img_path = self.path + img_name + \".jpg\"\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform)\n",
    "        img_processed = transform_valid(img)\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return img_processed\n",
    "\n",
    "    def __len__(self):\n",
    "        # total size of your dataset.\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network\n",
    "class MyENet(nn.Module):\n",
    "    def __init__(self, ENet):\n",
    "        super(MyENet, self).__init__()\n",
    "        # modify output layer of the pre-trained ENet \n",
    "        self.ENet = ENet\n",
    "        num_ftrs = self.ENet._fc.in_features\n",
    "        self.ENet._fc = nn.Linear(in_features=num_ftrs, out_features=1024)\n",
    "        # map Enet output to melanoma decision \n",
    "        self.output = nn.Sequential(nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=0.4),\n",
    "                                    nn.Linear(1024, 1),\n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        out = self.ENet(x)\n",
    "        return out \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.ENet(x)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Making predictions on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "test_path  = path + \"test/\"\n",
    "\n",
    "# define batch size and accumulation steps \n",
    "batchsize  = 8\n",
    "\n",
    "# record out-of-fold predictions and test predictions \n",
    "predictions = np.zeros(len(test_df))\n",
    "\n",
    "# create a test loader \n",
    "test_dataset = TestDataset(test_df, test_path)                                              \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batchsize)\n",
    "\n",
    "# set up model \n",
    "ENet = EfficientNet.from_pretrained('efficientnet-b0').to(device)\n",
    "model = MyENet(ENet).to(device)\n",
    "model.load_state_dict(torch.load(\"../Models/model_1.ckpt\"))\n",
    "\n",
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, images in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # append predictions\n",
    "        test_predictions += list(outputs.detach().cpu().numpy())\n",
    "test_predictions = np.array(test_predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = test_df['image_name'].values\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission[\"image_name\"] = image_names\n",
    "submission[\"target\"] = test_predictions\n",
    "submission.to_csv(\"m1_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0653e+04, 1.4200e+02, 7.2000e+01, 3.7000e+01, 1.7000e+01,\n",
       "        1.8000e+01, 1.3000e+01, 6.0000e+00, 9.0000e+00, 1.5000e+01]),\n",
       " array([7.1873674e-10, 9.9351443e-02, 1.9870289e-01, 2.9805434e-01,\n",
       "        3.9740577e-01, 4.9675721e-01, 5.9610868e-01, 6.9546008e-01,\n",
       "        7.9481155e-01, 8.9416295e-01, 9.9351442e-01], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
